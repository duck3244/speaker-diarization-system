# system_monitor.py
"""
ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”
"""

import torch
import psutil
import time
import gc
from typing import Dict, List, Optional
from dataclasses import dataclass
from datetime import datetime
import threading
import warnings

warnings.filterwarnings("ignore")


@dataclass
class SystemStats:
    """ì‹œìŠ¤í…œ í†µê³„ ì •ë³´"""
    timestamp: datetime
    gpu_memory_allocated: float  # GB
    gpu_memory_reserved: float  # GB
    gpu_memory_total: float  # GB
    gpu_utilization: float  # %
    cpu_usage: float  # %
    ram_usage: float  # %
    ram_available: float  # GB


class SystemMonitor:
    """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§"""

    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.stats_history: List[SystemStats] = []
        self.monitoring = False

    def get_current_stats(self) -> SystemStats:
        """í˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""

        timestamp = datetime.now()

        # GPU ì •ë³´
        if self.device == "cuda":
            try:
                gpu_memory_allocated = torch.cuda.memory_allocated(0) / 1024 ** 3
                gpu_memory_reserved = torch.cuda.memory_reserved(0) / 1024 ** 3
                gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1024 ** 3

                # GPU ì‚¬ìš©ë¥  (ê·¼ì‚¬ì¹˜)
                gpu_utilization = (gpu_memory_allocated / gpu_memory_total) * 100

            except Exception:
                gpu_memory_allocated = 0
                gpu_memory_reserved = 0
                gpu_memory_total = 0
                gpu_utilization = 0
        else:
            gpu_memory_allocated = 0
            gpu_memory_reserved = 0
            gpu_memory_total = 0
            gpu_utilization = 0

        # CPU ë° RAM ì •ë³´
        cpu_usage = psutil.cpu_percent(interval=1)
        memory_info = psutil.virtual_memory()
        ram_usage = memory_info.percent
        ram_available = memory_info.available / 1024 ** 3

        return SystemStats(
            timestamp=timestamp,
            gpu_memory_allocated=gpu_memory_allocated,
            gpu_memory_reserved=gpu_memory_reserved,
            gpu_memory_total=gpu_memory_total,
            gpu_utilization=gpu_utilization,
            cpu_usage=cpu_usage,
            ram_usage=ram_usage,
            ram_available=ram_available
        )

    def print_current_stats(self):
        """í˜„ì¬ ìƒíƒœ ì¶œë ¥"""

        stats = self.get_current_stats()

        print(f"\n=== ì‹œìŠ¤í…œ ìƒíƒœ ({stats.timestamp.strftime('%H:%M:%S')}) ===")

        if self.device == "cuda":
            print(f"GPU ë©”ëª¨ë¦¬:")
            print(f"  í• ë‹¹ë¨: {stats.gpu_memory_allocated:.2f}GB")
            print(f"  ì˜ˆì•½ë¨: {stats.gpu_memory_reserved:.2f}GB")
            print(f"  ì´ ìš©ëŸ‰: {stats.gpu_memory_total:.2f}GB")
            print(f"  ì‚¬ìš©ë¥ : {stats.gpu_utilization:.1f}%")

            # ë©”ëª¨ë¦¬ ì—¬ìœ ë„ ê²½ê³ 
            if stats.gpu_utilization > 90:
                print("âš ï¸ GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ìœ„í—˜")
            elif stats.gpu_utilization > 75:
                print("âš ï¸ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ")

        print(f"CPU ì‚¬ìš©ë¥ : {stats.cpu_usage:.1f}%")
        print(f"RAM ì‚¬ìš©ë¥ : {stats.ram_usage:.1f}%")
        print(f"RAM ì—¬ìœ : {stats.ram_available:.1f}GB")

        # CPU/RAM ê²½ê³ 
        if stats.cpu_usage > 90:
            print("âš ï¸ CPU ì‚¬ìš©ë¥  ë†’ìŒ")
        if stats.ram_usage > 90:
            print("âš ï¸ RAM ì‚¬ìš©ë¥  ë†’ìŒ")

    def start_monitoring(self, interval: float = 5.0):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ)"""

        if self.monitoring:
            print("ì´ë¯¸ ëª¨ë‹ˆí„°ë§ì´ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
            return

        self.monitoring = True
        print(f"ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ì‹œì‘ (ê°„ê²©: {interval}ì´ˆ)")

        def monitor_loop():
            try:
                while self.monitoring:
                    stats = self.get_current_stats()
                    self.stats_history.append(stats)

                    # ìµœê·¼ 100ê°œ ê¸°ë¡ë§Œ ìœ ì§€
                    if len(self.stats_history) > 100:
                        self.stats_history = self.stats_history[-100:]

                    time.sleep(interval)

            except Exception as e:
                print(f"ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                self.monitoring = False

        # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œë¡œ ì‹¤í–‰
        monitor_thread = threading.Thread(target=monitor_loop, daemon=True)
        monitor_thread.start()

    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ë‹¨"""
        self.monitoring = False

    def get_memory_trend(self, window_size: int = 10) -> Dict:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ íŠ¸ë Œë“œ ë¶„ì„"""

        if len(self.stats_history) < window_size:
            return {'trend': 'insufficient_data'}

        recent_stats = self.stats_history[-window_size:]

        # GPU ë©”ëª¨ë¦¬ íŠ¸ë Œë“œ
        gpu_memory_values = [s.gpu_memory_allocated for s in recent_stats]
        gpu_trend = self._calculate_trend(gpu_memory_values)

        # RAM íŠ¸ë Œë“œ
        ram_values = [s.ram_usage for s in recent_stats]
        ram_trend = self._calculate_trend(ram_values)

        return {
            'gpu_memory_trend': gpu_trend,
            'ram_trend': ram_trend,
            'window_size': window_size,
            'latest_gpu_memory': gpu_memory_values[-1],
            'latest_ram_usage': ram_values[-1]
        }

    def _calculate_trend(self, values: List[float]) -> str:
        """ê°’ë“¤ì˜ íŠ¸ë Œë“œ ê³„ì‚°"""

        if len(values) < 2:
            return 'stable'

        # ì„ í˜• íšŒê·€ë¡œ ê¸°ìš¸ê¸° ê³„ì‚°
        n = len(values)
        x = list(range(n))

        # ê¸°ìš¸ê¸° ê³„ì‚°
        x_mean = sum(x) / n
        y_mean = sum(values) / n

        numerator = sum((x[i] - x_mean) * (values[i] - y_mean) for i in range(n))
        denominator = sum((x[i] - x_mean) ** 2 for i in range(n))

        if denominator == 0:
            return 'stable'

        slope = numerator / denominator

        # íŠ¸ë Œë“œ ë¶„ë¥˜
        if slope > 0.1:
            return 'increasing'
        elif slope < -0.1:
            return 'decreasing'
        else:
            return 'stable'

    def get_performance_summary(self) -> Dict:
        """ì„±ëŠ¥ ìš”ì•½ ì •ë³´"""

        if not self.stats_history:
            return {'error': 'no_data'}

        latest = self.stats_history[-1]

        # í‰ê· ê°’ ê³„ì‚° (ìµœê·¼ 10ê°œ)
        recent_window = min(10, len(self.stats_history))
        recent_stats = self.stats_history[-recent_window:]

        avg_gpu_memory = sum(s.gpu_memory_allocated for s in recent_stats) / recent_window
        avg_cpu_usage = sum(s.cpu_usage for s in recent_stats) / recent_window
        avg_ram_usage = sum(s.ram_usage for s in recent_stats) / recent_window

        # ìµœëŒ€ê°’
        max_gpu_memory = max(s.gpu_memory_allocated for s in recent_stats)
        max_cpu_usage = max(s.cpu_usage for s in recent_stats)
        max_ram_usage = max(s.ram_usage for s in recent_stats)

        return {
            'current': {
                'gpu_memory_gb': latest.gpu_memory_allocated,
                'cpu_usage_pct': latest.cpu_usage,
                'ram_usage_pct': latest.ram_usage
            },
            'average': {
                'gpu_memory_gb': avg_gpu_memory,
                'cpu_usage_pct': avg_cpu_usage,
                'ram_usage_pct': avg_ram_usage
            },
            'peak': {
                'gpu_memory_gb': max_gpu_memory,
                'cpu_usage_pct': max_cpu_usage,
                'ram_usage_pct': max_ram_usage
            },
            'window_size': recent_window
        }


class MemoryManager:
    """ë©”ëª¨ë¦¬ ê´€ë¦¬ ë° ìµœì í™”"""

    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

    def cleanup_memory(self, verbose: bool = True):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""

        if verbose:
            print("ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")

        # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()

        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if self.device == "cuda":
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()

        if verbose:
            print("âœ… ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")

    def get_memory_info(self) -> Dict:
        """ë©”ëª¨ë¦¬ ì •ë³´ ì¡°íšŒ"""

        info = {}

        if self.device == "cuda":
            try:
                # GPU ë©”ëª¨ë¦¬ ì •ë³´
                allocated = torch.cuda.memory_allocated(0)
                reserved = torch.cuda.memory_reserved(0)
                total = torch.cuda.get_device_properties(0).total_memory

                info['gpu'] = {
                    'allocated_gb': allocated / 1024 ** 3,
                    'reserved_gb': reserved / 1024 ** 3,
                    'total_gb': total / 1024 ** 3,
                    'free_gb': (total - reserved) / 1024 ** 3,
                    'utilization_pct': (allocated / total) * 100
                }

                # GPU ëª¨ë¸ ì •ë³´
                info['gpu']['name'] = torch.cuda.get_device_name(0)
                info['gpu']['capability'] = torch.cuda.get_device_capability(0)

            except Exception as e:
                info['gpu'] = {'error': str(e)}

        # CPU ë° RAM ì •ë³´
        memory = psutil.virtual_memory()
        info['ram'] = {
            'total_gb': memory.total / 1024 ** 3,
            'available_gb': memory.available / 1024 ** 3,
            'used_gb': memory.used / 1024 ** 3,
            'usage_pct': memory.percent
        }

        info['cpu'] = {
            'usage_pct': psutil.cpu_percent(interval=1),
            'count': psutil.cpu_count(),
            'freq_mhz': psutil.cpu_freq().current if psutil.cpu_freq() else 0
        }

        return info

    def check_memory_availability(self, required_gb: float) -> Dict:
        """í•„ìš”í•œ ë©”ëª¨ë¦¬ê°€ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸"""

        info = self.get_memory_info()

        result = {
            'required_gb': required_gb,
            'available': False,
            'recommendations': []
        }

        if self.device == "cuda" and 'gpu' in info:
            gpu_info = info['gpu']
            if 'free_gb' in gpu_info:
                gpu_free = gpu_info['free_gb']

                if gpu_free >= required_gb:
                    result['available'] = True
                    result['gpu_free_gb'] = gpu_free
                else:
                    result['gpu_free_gb'] = gpu_free
                    result['gpu_shortage_gb'] = required_gb - gpu_free

                    # ê¶Œì¥ì‚¬í•­
                    if gpu_info['utilization_pct'] > 80:
                        result['recommendations'].append("ë©”ëª¨ë¦¬ ì •ë¦¬ ìˆ˜í–‰")
                    if required_gb > gpu_info['total_gb'] * 0.8:
                        result['recommendations'].append("ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©")
                        result['recommendations'].append("ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°")

        # RAM í™•ì¸
        ram_info = info['ram']
        ram_free = ram_info['available_gb']

        if not result['available']:  # GPUê°€ ë¶€ì¡±í•œ ê²½ìš° RAM ì‚¬ìš© ê³ ë ¤
            if ram_free >= required_gb:
                result['cpu_fallback_possible'] = True
                result['recommendations'].append("CPU ëª¨ë“œë¡œ ì „í™˜")
            else:
                result['cpu_fallback_possible'] = False
                result['recommendations'].append("ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ë¶€ì¡±")

        return result

    def optimize_for_model(self, model_name: str) -> Dict:
        """ëª¨ë¸ë³„ ë©”ëª¨ë¦¬ ìµœì í™”"""

        # ëª¨ë¸ë³„ ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (GB)
        model_memory_requirements = {
            "openai/whisper-base": 1.0,
            "openai/whisper-medium": 2.5,
            "openai/whisper-large-v3": 6.0,
            "pyannote/speaker-diarization": 2.0
        }

        required_memory = model_memory_requirements.get(model_name, 3.0)

        # í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
        availability = self.check_memory_availability(required_memory)

        optimization = {
            'model': model_name,
            'required_memory_gb': required_memory,
            'memory_available': availability['available'],
            'optimizations_applied': []
        }

        if not availability['available']:
            # ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œë„
            self.cleanup_memory()
            optimization['optimizations_applied'].append("memory_cleanup")

            # ì¬í™•ì¸
            availability = self.check_memory_availability(required_memory)
            optimization['memory_available_after_cleanup'] = availability['available']

            if not availability['available']:
                # ì¶”ê°€ ìµœì í™” ì œì•ˆ
                optimization['suggestions'] = availability['recommendations']

        return optimization


class ProcessingTimer:
    """ì²˜ë¦¬ ì‹œê°„ ì¸¡ì • ë° ë¶„ì„"""

    def __init__(self):
        self.timings: Dict[str, List[float]] = {}
        self.start_times: Dict[str, float] = {}

    def start(self, operation_name: str):
        """íƒ€ì´ë° ì‹œì‘"""
        self.start_times[operation_name] = time.time()

    def end(self, operation_name: str) -> float:
        """íƒ€ì´ë° ì¢…ë£Œ ë° ê¸°ë¡"""

        if operation_name not in self.start_times:
            raise ValueError(f"íƒ€ì´ë°ì´ ì‹œì‘ë˜ì§€ ì•ŠìŒ: {operation_name}")

        elapsed = time.time() - self.start_times[operation_name]

        if operation_name not in self.timings:
            self.timings[operation_name] = []

        self.timings[operation_name].append(elapsed)
        del self.start_times[operation_name]

        return elapsed

    def get_statistics(self, operation_name: str) -> Dict:
        """ì‘ì—…ë³„ í†µê³„"""

        if operation_name not in self.timings:
            return {'error': 'no_data'}

        times = self.timings[operation_name]

        return {
            'count': len(times),
            'total_time': sum(times),
            'average_time': sum(times) / len(times),
            'min_time': min(times),
            'max_time': max(times),
            'last_time': times[-1] if times else 0
        }

    def get_all_statistics(self) -> Dict:
        """ëª¨ë“  ì‘ì—… í†µê³„"""

        all_stats = {}

        for operation in self.timings:
            all_stats[operation] = self.get_statistics(operation)

        return all_stats

    def print_summary(self):
        """í†µê³„ ìš”ì•½ ì¶œë ¥"""

        print("\n=== ì²˜ë¦¬ ì‹œê°„ í†µê³„ ===")

        for operation, stats in self.get_all_statistics().items():
            if 'error' not in stats:
                print(f"\n{operation}:")
                print(f"  íšŸìˆ˜: {stats['count']}")
                print(f"  í‰ê· : {stats['average_time']:.2f}ì´ˆ")
                print(f"  ìµœì†Œ: {stats['min_time']:.2f}ì´ˆ")
                print(f"  ìµœëŒ€: {stats['max_time']:.2f}ì´ˆ")
                print(f"  ì´ ì‹œê°„: {stats['total_time']:.2f}ì´ˆ")


class ResourceMonitor:
    """ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""

    def __init__(self, operation_name: str = "operation"):
        self.operation_name = operation_name
        self.monitor = SystemMonitor()
        self.timer = ProcessingTimer()
        self.memory_manager = MemoryManager()

        self.start_stats = None
        self.end_stats = None

    def __enter__(self):
        """ì»¨í…ìŠ¤íŠ¸ ì‹œì‘"""
        # ì‹œì‘ ì‹œì  ê¸°ë¡
        self.start_stats = self.monitor.get_current_stats()
        self.timer.start(self.operation_name)

        print(f"ğŸš€ {self.operation_name} ì‹œì‘")
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """ì»¨í…ìŠ¤íŠ¸ ì¢…ë£Œ"""
        # ì¢…ë£Œ ì‹œì  ê¸°ë¡
        try:
            elapsed_time = self.timer.end(self.operation_name)
            self.end_stats = self.monitor.get_current_stats()

            # ê²°ê³¼ ì¶œë ¥
            print(f"âœ… {self.operation_name} ì™„ë£Œ ({elapsed_time:.2f}ì´ˆ)")

        except Exception as timer_error:
            print(f"âš ï¸ íƒ€ì´ë¨¸ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {timer_error}")
            elapsed_time = 0
            self.end_stats = self.monitor.get_current_stats()

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë³€í™” ê³„ì‚°
        if self.start_stats and self.end_stats:
            gpu_memory_diff = self.end_stats.gpu_memory_allocated - self.start_stats.gpu_memory_allocated
            ram_diff = self.end_stats.ram_usage - self.start_stats.ram_usage

            print(f"ë©”ëª¨ë¦¬ ë³€í™”:")
            print(f"  GPU: {gpu_memory_diff:+.2f}GB")
            print(f"  RAM: {ram_diff:+.1f}%")

            # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê²½ê³ 
            if gpu_memory_diff > 1.0:
                print("âš ï¸ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í¬ê²Œ ì¦ê°€")
            if ram_diff > 10:
                print("âš ï¸ RAM ì‚¬ìš©ëŸ‰ í¬ê²Œ ì¦ê°€")

        # ì˜ˆì™¸ê°€ ë°œìƒí•œ ê²½ìš°ì—ë„ ì •ìƒì ìœ¼ë¡œ ì²˜ë¦¬
        if exc_type is not None:
            print(f"âŒ {self.operation_name} ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {exc_val}")
            # ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹œë„
            try:
                self.memory_manager.cleanup_memory(verbose=False)
            except Exception as cleanup_error:
                print(f"âš ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {cleanup_error}")

            # ì˜ˆì™¸ë¥¼ ì–µì œí•˜ì§€ ì•Šê³  ì „íŒŒ (False ë°˜í™˜)
            return False

        # ì •ìƒ ì¢…ë£Œ
        return True

    def get_resource_usage(self) -> Dict:
        """ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ë°˜í™˜"""

        if not (self.start_stats and self.end_stats):
            return {'error': 'incomplete_monitoring'}

        # timerì—ì„œ í†µê³„ ê°€ì ¸ì˜¤ê¸° ì‹œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        try:
            duration_stats = self.timer.get_statistics(self.operation_name)
            if 'error' in duration_stats:
                duration_stats = {'last_time': 0, 'average_time': 0}
        except Exception:
            duration_stats = {'last_time': 0, 'average_time': 0}

        return {
            'operation': self.operation_name,
            'duration': duration_stats,
            'memory_change': {
                'gpu_gb': self.end_stats.gpu_memory_allocated - self.start_stats.gpu_memory_allocated,
                'ram_pct': self.end_stats.ram_usage - self.start_stats.ram_usage
            },
            'final_usage': {
                'gpu_gb': self.end_stats.gpu_memory_allocated,
                'ram_pct': self.end_stats.ram_usage,
                'cpu_pct': self.end_stats.cpu_usage
            }
        }


class PerformanceAnalyzer:
    """ì„±ëŠ¥ ë¶„ì„ ë° ìµœì í™” ì œì•ˆ"""

    def __init__(self):
        self.benchmark_results = {}

    def analyze_gpu_compatibility(self) -> Dict:
        """GPU í˜¸í™˜ì„± ë° ìµœì í™” ë¶„ì„"""

        analysis = {
            'gpu_detected': torch.cuda.is_available(),
            'recommendations': []
        }

        if not torch.cuda.is_available():
            analysis['recommendations'] = [
                "CUDAê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ê±°ë‚˜ GPUë¥¼ ê°ì§€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                "CPU ëª¨ë“œë¡œ ì‹¤í–‰í•˜ê±°ë‚˜ GPU ë“œë¼ì´ë²„ë¥¼ í™•ì¸í•˜ì„¸ìš”"
            ]
            return analysis

        try:
            gpu_name = torch.cuda.get_device_name(0)
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024 ** 3

            analysis['gpu_name'] = gpu_name
            analysis['total_memory_gb'] = total_memory

            # GPUë³„ ìµœì í™” ì œì•ˆ
            if "4060" in gpu_name:
                analysis['gpu_tier'] = "mid_range"
                analysis['recommended_models'] = ["whisper-base", "whisper-medium"]
                analysis['max_batch_size'] = 2
                analysis['recommendations'] = [
                    "RTX 4060 ê°ì§€ë¨ - medium ëª¨ë¸ ê¶Œì¥",
                    "FP16 ì •ë°€ë„ ì‚¬ìš©ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½",
                    "ë°°ì¹˜ í¬ê¸°ëŠ” 2 ì´í•˜ë¡œ ì œí•œ",
                    "ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬ë¡œ ê¸´ ì˜¤ë””ì˜¤ ì•ˆì „ ì²˜ë¦¬"
                ]
            elif "3090" in gpu_name or "4090" in gpu_name:
                analysis['gpu_tier'] = "high_end"
                analysis['recommended_models'] = ["whisper-large-v3"]
                analysis['max_batch_size'] = 8
                analysis['recommendations'] = [
                    "ê³ ì„±ëŠ¥ GPU ê°ì§€ë¨ - large ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥",
                    "ë°°ì¹˜ í¬ê¸° ìµœëŒ€ 8ê¹Œì§€ ì‚¬ìš© ê°€ëŠ¥",
                    "ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥"
                ]
            elif "A100" in gpu_name:
                analysis['gpu_tier'] = "enterprise"
                analysis['recommended_models'] = ["whisper-large-v3"]
                analysis['max_batch_size'] = 16
                analysis['recommendations'] = [
                    "ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ GPU - ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì‚¬ìš©",
                    "ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ê°€ëŠ¥",
                    "ë©€í‹° GPU í™œìš© ê³ ë ¤"
                ]
            else:
                # ë©”ëª¨ë¦¬ ê¸°ë°˜ ì¶”ì •
                if total_memory < 8:
                    analysis['gpu_tier'] = "low_end"
                    analysis['recommended_models'] = ["whisper-base"]
                    analysis['max_batch_size'] = 1
                elif total_memory < 16:
                    analysis['gpu_tier'] = "mid_range"
                    analysis['recommended_models'] = ["whisper-medium"]
                    analysis['max_batch_size'] = 4
                else:
                    analysis['gpu_tier'] = "high_end"
                    analysis['recommended_models'] = ["whisper-large-v3"]
                    analysis['max_batch_size'] = 8

            # ë©”ëª¨ë¦¬ ìµœì í™” ì œì•ˆ
            if total_memory < 12:
                analysis['recommendations'].extend([
                    "ë©”ëª¨ë¦¬ ì œí•œ GPU - ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬ í•„ìš”",
                    "ê¸´ ì˜¤ë””ì˜¤ëŠ” ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬"
                ])

        except Exception as e:
            analysis['error'] = str(e)
            analysis['recommendations'] = ["GPU ì •ë³´ í™•ì¸ ì‹¤íŒ¨ - CPU ëª¨ë“œ ì‚¬ìš©"]

        return analysis

    def benchmark_model_loading(self, models: List[str]) -> Dict:
        """ëª¨ë¸ ë¡œë”© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""

        results = {}

        for model_name in models:
            print(f"ë²¤ì¹˜ë§ˆí‚¹: {model_name}")

            try:
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

                # ë¡œë”© ì‹œê°„ ì¸¡ì •
                start_time = time.time()

                # ê°„ë‹¨í•œ ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸
                from transformers import WhisperProcessor
                processor = WhisperProcessor.from_pretrained(model_name)

                load_time = time.time() - start_time

                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
                if torch.cuda.is_available():
                    memory_used = torch.cuda.memory_allocated(0) / 1024 ** 3
                else:
                    memory_used = 0

                results[model_name] = {
                    'load_time_seconds': load_time,
                    'memory_usage_gb': memory_used,
                    'success': True
                }

                # ì •ë¦¬
                del processor

            except Exception as e:
                results[model_name] = {
                    'success': False,
                    'error': str(e)
                }

            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        return results

    def suggest_optimal_config(self, audio_duration: float,
                               num_speakers: int = None) -> Dict:
        """ìµœì  ì„¤ì • ì œì•ˆ"""

        gpu_analysis = self.analyze_gpu_compatibility()

        config = {
            'audio_duration': audio_duration,
            'estimated_speakers': num_speakers,
            'processing_strategy': 'sequential'  # ê¸°ë³¸ê°’
        }

        # GPU ê¸°ë°˜ ëª¨ë¸ ì„ íƒ
        if gpu_analysis['gpu_detected'] and 'recommended_models' in gpu_analysis:
            config['recommended_model'] = gpu_analysis['recommended_models'][0]
            config['max_batch_size'] = gpu_analysis.get('max_batch_size', 2)
        else:
            config['recommended_model'] = "openai/whisper-base"
            config['max_batch_size'] = 1
            config['use_cpu'] = True

        # ì˜¤ë””ì˜¤ ê¸¸ì´ ê¸°ë°˜ ìµœì í™”
        if audio_duration > 300:  # 5ë¶„ ì´ìƒ
            config['processing_strategy'] = 'chunked'
            config['chunk_duration'] = 60
            config['recommendations'] = [
                "ê¸´ ì˜¤ë””ì˜¤ ê°ì§€ë¨ - ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬",
                "ì¤‘ê°„ ê²°ê³¼ ì €ì¥ìœ¼ë¡œ ì•ˆì •ì„± í–¥ìƒ"
            ]
        elif audio_duration > 60:  # 1ë¶„ ì´ìƒ
            config['processing_strategy'] = 'chunked'
            config['chunk_duration'] = 30
        else:
            config['processing_strategy'] = 'full'

        # í™”ì ìˆ˜ ê¸°ë°˜ ìµœì í™”
        if num_speakers and num_speakers > 3:
            config['diarization_method'] = 'pyannote'
            config['clustering_method'] = 'agglomerative'
        else:
            config['diarization_method'] = 'lightweight'
            config['clustering_method'] = 'kmeans'

        # ë©”ëª¨ë¦¬ ê¸°ë°˜ ì¡°ì •
        memory_info = MemoryManager().get_memory_info()

        if torch.cuda.is_available() and 'gpu' in memory_info:
            available_memory = memory_info['gpu'].get('free_gb', 0)

            if available_memory < 2:
                config['force_cpu'] = True
                config['recommendations'] = config.get('recommendations', [])
                config['recommendations'].append("GPU ë©”ëª¨ë¦¬ ë¶€ì¡± - CPU ëª¨ë“œ ê¶Œì¥")
            elif available_memory < 4:
                config['use_fp16'] = True
                config['recommendations'] = config.get('recommendations', [])
                config['recommendations'].append("FP16 ì •ë°€ë„ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½")

        return config


def quick_system_check() -> Dict:
    """ë¹ ë¥¸ ì‹œìŠ¤í…œ ì²´í¬"""

    monitor = SystemMonitor()
    memory_manager = MemoryManager()
    analyzer = PerformanceAnalyzer()

    return {
        'system_stats': monitor.get_current_stats(),
        'memory_info': memory_manager.get_memory_info(),
        'gpu_analysis': analyzer.analyze_gpu_compatibility(),
        'timestamp': datetime.now()
    }


def print_system_info():
    """ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥"""

    info = quick_system_check()

    print("=== ì‹œìŠ¤í…œ ì •ë³´ ===")

    # GPU ì •ë³´
    gpu_analysis = info['gpu_analysis']
    if gpu_analysis['gpu_detected']:
        print(f"GPU: {gpu_analysis.get('gpu_name', 'Unknown')}")
        print(f"ë©”ëª¨ë¦¬: {gpu_analysis.get('total_memory_gb', 0):.1f}GB")
        print(f"ë“±ê¸‰: {gpu_analysis.get('gpu_tier', 'unknown')}")
        print(f"ê¶Œì¥ ëª¨ë¸: {', '.join(gpu_analysis.get('recommended_models', []))}")
    else:
        print("GPU: ê°ì§€ë˜ì§€ ì•ŠìŒ (CPU ëª¨ë“œ)")

    # ë©”ëª¨ë¦¬ ì •ë³´
    memory_info = info['memory_info']
    print(f"\nRAM: {memory_info['ram']['total_gb']:.1f}GB")
    print(f"RAM ì‚¬ìš©ë¥ : {memory_info['ram']['usage_pct']:.1f}%")
    print(f"CPU ì‚¬ìš©ë¥ : {memory_info['cpu']['usage_pct']:.1f}%")

    # ê¶Œì¥ì‚¬í•­
    if 'recommendations' in gpu_analysis:
        print(f"\nê¶Œì¥ì‚¬í•­:")
        for rec in gpu_analysis['recommendations']:
            print(f"  â€¢ {rec}")


def monitor_processing(operation_name: str):
    """ì²˜ë¦¬ ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°"""

    def decorator(func):
        def wrapper(*args, **kwargs):
            with ResourceMonitor(operation_name) as monitor:
                result = func(*args, **kwargs)

                # ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì •ë³´ë¥¼ ê²°ê³¼ì— ì¶”ê°€ (ê°€ëŠ¥í•œ ê²½ìš°)
                if isinstance(result, dict):
                    result['_resource_usage'] = monitor.get_resource_usage()

                return result

        return wrapper

    return decorator


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ëª¨ë“ˆ ë¡œë“œë¨")

    # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
    print_system_info()

    # ì„±ëŠ¥ ë¶„ì„ ì˜ˆì‹œ
    analyzer = PerformanceAnalyzer()

    # 1ì‹œê°„ ì˜¤ë””ì˜¤, 3ëª… í™”ì ê°€ì •
    optimal_config = analyzer.suggest_optimal_config(
        audio_duration=3600,  # 1ì‹œê°„
        num_speakers=3
    )

    print(f"\n=== ìµœì  ì„¤ì • ì œì•ˆ ===")
    print(f"ì²˜ë¦¬ ì „ëµ: {optimal_config['processing_strategy']}")
    print(f"ê¶Œì¥ ëª¨ë¸: {optimal_config['recommended_model']}")
    print(f"ë°°ì¹˜ í¬ê¸°: {optimal_config['max_batch_size']}")

    if 'recommendations' in optimal_config:
        print("ì¶”ê°€ ê¶Œì¥ì‚¬í•­:")
        for rec in optimal_config['recommendations']:
            print(f"  â€¢ {rec}")